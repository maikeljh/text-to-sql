{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "notebook_dir = os.getcwd()\n",
    "parent_dir = os.path.abspath(os.path.join(notebook_dir, '..'))\n",
    "\n",
    "if parent_dir not in sys.path:\n",
    "    sys.path.insert(0, parent_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from text_to_sql import (\n",
    "    TextToSQL,\n",
    "    Config,\n",
    "    LLMConfig,\n",
    "    SLConfig,\n",
    "    ContextConfig,\n",
    "    QueryConfig,\n",
    ")\n",
    "from dotenv import load_dotenv\n",
    "from datetime import datetime\n",
    "\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_RETRIES = 5\n",
    "RETRY_DELAY = 2\n",
    "DATABASE = \"sakila\"\n",
    "MODEL = \"gpt-4.1-mini\"\n",
    "PROVIDER = \"openai\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Timestamp Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp = datetime.now().strftime(\"%Y_%m_%d_%H_%M\")\n",
    "output_dir = f\"../files/experiment_result/{timestamp}\"\n",
    "os.makedirs(output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_key = DATABASE.upper().replace(\"-\", \"_\")\n",
    "provider_key = PROVIDER.upper().replace(\"-\", \"_\")\n",
    "\n",
    "config = Config(\n",
    "    max_retry_attempt=5,\n",
    "    rewriter_config=LLMConfig(\n",
    "        type=\"api\",\n",
    "        model=MODEL,\n",
    "        provider=PROVIDER,\n",
    "        api_key=os.getenv(f\"API_KEY_{provider_key}\"),\n",
    "    ),\n",
    "    query_generator_config=LLMConfig(\n",
    "        type=\"api\",\n",
    "        model=MODEL,\n",
    "        provider=PROVIDER,\n",
    "        api_key=os.getenv(f\"API_KEY_{provider_key}\"),\n",
    "    ),\n",
    "    schema_linker_config=SLConfig(\n",
    "        type=\"api\",\n",
    "        model=MODEL,\n",
    "        provider=PROVIDER,\n",
    "        api_key=os.getenv(f\"API_KEY_{provider_key}\"),\n",
    "        schema_path=f\"../files/schema/{DATABASE}.txt\",\n",
    "        metadata_path=f\"../files/metadata/{DATABASE}.json\",\n",
    "    ),\n",
    "    retrieve_context_config=ContextConfig(data_path=f\"../files/dataset/dataset_{DATABASE}_example.csv\"),\n",
    "    query_executor_config = QueryConfig(\n",
    "        host=os.getenv(f\"DB_HOST_{db_key}\"),\n",
    "        database=os.getenv(f\"DB_DATABASE_{db_key}\"),\n",
    "        user=os.getenv(f\"DB_USER_{db_key}\"),\n",
    "        password=os.getenv(f\"DB_PASSWORD_{db_key}\"),\n",
    "        port=os.getenv(f\"DB_PORT_{db_key}\"),\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_to_sql_model = TextToSQL(config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(f\"../files/dataset/dataset_{DATABASE}_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import ast\n",
    "\n",
    "EA = 0\n",
    "total_questions = len(dataset) * 2\n",
    "results_list = []\n",
    "\n",
    "for idx, row in dataset.iterrows():\n",
    "    question_1 = row[\"Alternative Prompt 1 (English)\"]\n",
    "    question_2 = row[\"Alternative Prompt 2 (Bahasa Indonesia)\"]\n",
    "    answer = row[\"Answer\"]\n",
    "    expected_columns = ast.literal_eval(row[\"Expected Result\"])\n",
    "\n",
    "    for prompt_id, question in enumerate([question_1, question_2], start=1):\n",
    "        print(f\"\\nProcessing Question {idx + 1}.{prompt_id}: {question}\")\n",
    "        result = None\n",
    "\n",
    "        for attempt in range(1, MAX_RETRIES + 1):\n",
    "            try:\n",
    "                result = text_to_sql_model.generate_baseline(user_prompt=question)\n",
    "                break\n",
    "            except Exception as e:\n",
    "                print(f\"[Attempt {attempt}] Failed to generate SQL: {e}\")\n",
    "                if attempt < MAX_RETRIES:\n",
    "                    time.sleep(RETRY_DELAY)\n",
    "                else:\n",
    "                    print(\"Max retries reached. Setting result as 'ERROR'\")\n",
    "                    result = \"ERROR\"\n",
    "\n",
    "        print(f\"Generated SQL Query: {result}\")\n",
    "\n",
    "        try:\n",
    "            acc = text_to_sql_model.evaluate(query=result, true_query=answer, expected_columns=expected_columns)\n",
    "        except Exception as e:\n",
    "            print(f\"Evaluation failed: {e}\")\n",
    "            acc = 0.0\n",
    "\n",
    "        print(f\"Execution Accuracy: {acc:.4f}\")\n",
    "\n",
    "        results_list.append({\n",
    "            \"Question ID\": f\"{idx + 1}.{prompt_id}\",\n",
    "            \"Question\": question,\n",
    "            \"Generated SQL Query\": result,\n",
    "            \"Expected SQL Query\": answer,\n",
    "            \"Execution Accuracy\": acc\n",
    "        })\n",
    "\n",
    "        EA += acc\n",
    "\n",
    "# Calculate final execution accuracy\n",
    "final_accuracy = EA / total_questions if total_questions > 0 else 0\n",
    "print(f\"\\nFinal Execution Accuracy: {final_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results_baseline = pd.DataFrame(results_list)\n",
    "df_results_baseline.to_csv(f\"{output_dir}/{MODEL}_{DATABASE}_baseline.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment V1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import ast\n",
    "\n",
    "EA = 0\n",
    "total_questions = len(dataset) * 2\n",
    "results_list = []\n",
    "\n",
    "for idx, row in dataset.iterrows():\n",
    "    question_1 = row[\"Alternative Prompt 1 (English)\"]\n",
    "    question_2 = row[\"Alternative Prompt 2 (Bahasa Indonesia)\"]\n",
    "    answer = row[\"Answer\"]\n",
    "    expected_columns = ast.literal_eval(row[\"Expected Result\"])\n",
    "\n",
    "    for prompt_id, question in enumerate([question_1, question_2], start=1):\n",
    "        print(f\"\\nProcessing Question {idx + 1}.{prompt_id}: {question}\")\n",
    "        result = None\n",
    "\n",
    "        for attempt in range(1, MAX_RETRIES + 1):\n",
    "            try:\n",
    "                result = text_to_sql_model.generate_v1(user_prompt=question)\n",
    "                break\n",
    "            except Exception as e:\n",
    "                print(f\"[Attempt {attempt}] Failed to generate SQL: {e}\")\n",
    "                if attempt < MAX_RETRIES:\n",
    "                    time.sleep(RETRY_DELAY)\n",
    "                else:\n",
    "                    print(\"Max retries reached. Setting result as 'ERROR'\")\n",
    "                    result = \"ERROR\"\n",
    "\n",
    "        print(f\"Generated SQL Query: {result}\")\n",
    "\n",
    "        try:\n",
    "            acc = text_to_sql_model.evaluate(query=result, true_query=answer, expected_columns=expected_columns)\n",
    "        except Exception as e:\n",
    "            print(f\"Evaluation failed: {e}\")\n",
    "            acc = 0.0\n",
    "\n",
    "        print(f\"Execution Accuracy: {acc:.4f}\")\n",
    "\n",
    "        results_list.append({\n",
    "            \"Question ID\": f\"{idx + 1}.{prompt_id}\",\n",
    "            \"Question\": question,\n",
    "            \"Generated SQL Query\": result,\n",
    "            \"Expected SQL Query\": answer,\n",
    "            \"Execution Accuracy\": acc\n",
    "        })\n",
    "\n",
    "        EA += acc\n",
    "\n",
    "# Calculate final execution accuracy\n",
    "final_accuracy = EA / total_questions if total_questions > 0 else 0\n",
    "print(f\"\\nFinal Execution Accuracy: {final_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results_v1 = pd.DataFrame(results_list)\n",
    "df_results_v1.to_csv(f\"{output_dir}/{MODEL}_{DATABASE}_v1.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment V2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import ast\n",
    "\n",
    "EA = 0\n",
    "total_questions = len(dataset) * 2\n",
    "results_list = []\n",
    "\n",
    "for idx, row in dataset.iterrows():\n",
    "    question_1 = row[\"Alternative Prompt 1 (English)\"]\n",
    "    question_2 = row[\"Alternative Prompt 2 (Bahasa Indonesia)\"]\n",
    "    answer = row[\"Answer\"]\n",
    "    expected_columns = ast.literal_eval(row[\"Expected Result\"])\n",
    "\n",
    "    for prompt_id, question in enumerate([question_1, question_2], start=1):\n",
    "        print(f\"\\nProcessing Question {idx + 1}.{prompt_id}: {question}\")\n",
    "        result = None\n",
    "\n",
    "        for attempt in range(1, MAX_RETRIES + 1):\n",
    "            try:\n",
    "                result = text_to_sql_model.generate_v2(user_prompt=question)\n",
    "                break\n",
    "            except Exception as e:\n",
    "                print(f\"[Attempt {attempt}] Failed to generate SQL: {e}\")\n",
    "                if attempt < MAX_RETRIES:\n",
    "                    time.sleep(RETRY_DELAY)\n",
    "                else:\n",
    "                    print(\"Max retries reached. Setting result as 'ERROR'\")\n",
    "                    result = \"ERROR\"\n",
    "\n",
    "        print(f\"Generated SQL Query: {result}\")\n",
    "\n",
    "        try:\n",
    "            acc = text_to_sql_model.evaluate(query=result, true_query=answer, expected_columns=expected_columns)\n",
    "        except Exception as e:\n",
    "            print(f\"Evaluation failed: {e}\")\n",
    "            acc = 0.0\n",
    "\n",
    "        print(f\"Execution Accuracy: {acc:.4f}\")\n",
    "\n",
    "        results_list.append({\n",
    "            \"Question ID\": f\"{idx + 1}.{prompt_id}\",\n",
    "            \"Question\": question,\n",
    "            \"Generated SQL Query\": result,\n",
    "            \"Expected SQL Query\": answer,\n",
    "            \"Execution Accuracy\": acc\n",
    "        })\n",
    "\n",
    "        EA += acc\n",
    "\n",
    "# Calculate final execution accuracy\n",
    "final_accuracy = EA / total_questions if total_questions > 0 else 0\n",
    "print(f\"\\nFinal Execution Accuracy: {final_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results_v2 = pd.DataFrame(results_list)\n",
    "df_results_v2.to_csv(f\"{output_dir}/{MODEL}_{DATABASE}_v2.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment V3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import ast\n",
    "\n",
    "EA = 0\n",
    "total_questions = len(dataset) * 2\n",
    "results_list = []\n",
    "\n",
    "for idx, row in dataset.iterrows():\n",
    "    question_1 = row[\"Alternative Prompt 1 (English)\"]\n",
    "    question_2 = row[\"Alternative Prompt 2 (Bahasa Indonesia)\"]\n",
    "    answer = row[\"Answer\"]\n",
    "    expected_columns = ast.literal_eval(row[\"Expected Result\"])\n",
    "\n",
    "    for prompt_id, question in enumerate([question_1, question_2], start=1):\n",
    "        print(f\"\\nProcessing Question {idx + 1}.{prompt_id}: {question}\")\n",
    "        result = None\n",
    "\n",
    "        for attempt in range(1, MAX_RETRIES + 1):\n",
    "            try:\n",
    "                result = text_to_sql_model.generate_v3(user_prompt=question)\n",
    "                break\n",
    "            except Exception as e:\n",
    "                print(f\"[Attempt {attempt}] Failed to generate SQL: {e}\")\n",
    "                if attempt < MAX_RETRIES:\n",
    "                    time.sleep(RETRY_DELAY)\n",
    "                else:\n",
    "                    print(\"Max retries reached. Setting result as 'ERROR'\")\n",
    "                    result = \"ERROR\"\n",
    "\n",
    "        print(f\"Generated SQL Query: {result}\")\n",
    "\n",
    "        try:\n",
    "            acc = text_to_sql_model.evaluate(query=result, true_query=answer, expected_columns=expected_columns)\n",
    "        except Exception as e:\n",
    "            print(f\"Evaluation failed: {e}\")\n",
    "            acc = 0.0\n",
    "\n",
    "        print(f\"Execution Accuracy: {acc:.4f}\")\n",
    "\n",
    "        results_list.append({\n",
    "            \"Question ID\": f\"{idx + 1}.{prompt_id}\",\n",
    "            \"Question\": question,\n",
    "            \"Generated SQL Query\": result,\n",
    "            \"Expected SQL Query\": answer,\n",
    "            \"Execution Accuracy\": acc\n",
    "        })\n",
    "\n",
    "        EA += acc\n",
    "\n",
    "# Calculate final execution accuracy\n",
    "final_accuracy = EA / total_questions if total_questions > 0 else 0\n",
    "print(f\"\\nFinal Execution Accuracy: {final_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results_v3 = pd.DataFrame(results_list)\n",
    "df_results_v3.to_csv(f\"{output_dir}/{MODEL}_{DATABASE}_v3.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eksperimen V4 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import ast\n",
    "\n",
    "EA = 0\n",
    "total_questions = len(dataset) * 2\n",
    "results_list = []\n",
    "\n",
    "for idx, row in dataset.iterrows():\n",
    "    question_1 = row[\"Alternative Prompt 1 (English)\"]\n",
    "    question_2 = row[\"Alternative Prompt 2 (Bahasa Indonesia)\"]\n",
    "    answer = row[\"Answer\"]\n",
    "    expected_columns = ast.literal_eval(row[\"Expected Result\"])\n",
    "\n",
    "    for prompt_id, question in enumerate([question_1, question_2], start=1):\n",
    "        print(f\"\\nProcessing Question {idx + 1}.{prompt_id}: {question}\")\n",
    "        result = None\n",
    "\n",
    "        for attempt in range(1, MAX_RETRIES + 1):\n",
    "            try:\n",
    "                result = text_to_sql_model.generate_v4(user_prompt=question)\n",
    "                break\n",
    "            except Exception as e:\n",
    "                print(f\"[Attempt {attempt}] Failed to generate SQL: {e}\")\n",
    "                if attempt < MAX_RETRIES:\n",
    "                    time.sleep(RETRY_DELAY)\n",
    "                else:\n",
    "                    print(\"Max retries reached. Setting result as 'ERROR'\")\n",
    "                    result = \"ERROR\"\n",
    "\n",
    "        print(f\"Generated SQL Query: {result}\")\n",
    "\n",
    "        try:\n",
    "            acc = text_to_sql_model.evaluate(query=result, true_query=answer, expected_columns=expected_columns)\n",
    "        except Exception as e:\n",
    "            print(f\"Evaluation failed: {e}\")\n",
    "            acc = 0.0\n",
    "\n",
    "        print(f\"Execution Accuracy: {acc:.4f}\")\n",
    "\n",
    "        results_list.append({\n",
    "            \"Question ID\": f\"{idx + 1}.{prompt_id}\",\n",
    "            \"Question\": question,\n",
    "            \"Generated SQL Query\": result,\n",
    "            \"Expected SQL Query\": answer,\n",
    "            \"Execution Accuracy\": acc\n",
    "        })\n",
    "\n",
    "        EA += acc\n",
    "\n",
    "# Calculate final execution accuracy\n",
    "final_accuracy = EA / total_questions if total_questions > 0 else 0\n",
    "print(f\"\\nFinal Execution Accuracy: {final_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results_v4 = pd.DataFrame(results_list)\n",
    "df_results_v4.to_csv(f\"{output_dir}/{MODEL}_{DATABASE}_v4.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eksperimen V5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import ast\n",
    "\n",
    "EA = 0\n",
    "total_questions = len(dataset) * 2\n",
    "results_list = []\n",
    "\n",
    "for idx, row in dataset.iterrows():\n",
    "    question_1 = row[\"Alternative Prompt 1 (English)\"]\n",
    "    question_2 = row[\"Alternative Prompt 2 (Bahasa Indonesia)\"]\n",
    "    answer = row[\"Answer\"]\n",
    "    expected_columns = ast.literal_eval(row[\"Expected Result\"])\n",
    "\n",
    "    for prompt_id, question in enumerate([question_1, question_2], start=1):\n",
    "        print(f\"\\nProcessing Question {idx + 1}.{prompt_id}: {question}\")\n",
    "        result = None\n",
    "\n",
    "        for attempt in range(1, MAX_RETRIES + 1):\n",
    "            try:\n",
    "                result = text_to_sql_model.generate_v5(user_prompt=question)\n",
    "                break\n",
    "            except Exception as e:\n",
    "                print(f\"[Attempt {attempt}] Failed to generate SQL: {e}\")\n",
    "                if attempt < MAX_RETRIES:\n",
    "                    time.sleep(RETRY_DELAY)\n",
    "                else:\n",
    "                    print(\"Max retries reached. Setting result as 'ERROR'\")\n",
    "                    result = \"ERROR\"\n",
    "\n",
    "        print(f\"Generated SQL Query: {result}\")\n",
    "\n",
    "        try:\n",
    "            acc = text_to_sql_model.evaluate(query=result, true_query=answer, expected_columns=expected_columns)\n",
    "        except Exception as e:\n",
    "            print(f\"Evaluation failed: {e}\")\n",
    "            acc = 0.0\n",
    "\n",
    "        print(f\"Execution Accuracy: {acc:.4f}\")\n",
    "\n",
    "        results_list.append({\n",
    "            \"Question ID\": f\"{idx + 1}.{prompt_id}\",\n",
    "            \"Question\": question,\n",
    "            \"Generated SQL Query\": result,\n",
    "            \"Expected SQL Query\": answer,\n",
    "            \"Execution Accuracy\": acc\n",
    "        })\n",
    "\n",
    "        EA += acc\n",
    "\n",
    "# Calculate final execution accuracy\n",
    "final_accuracy = EA / total_questions if total_questions > 0 else 0\n",
    "print(f\"\\nFinal Execution Accuracy: {final_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results_v5 = pd.DataFrame(results_list)\n",
    "df_results_v5.to_csv(f\"{output_dir}/{MODEL}_{DATABASE}_v5.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
