{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b78c2f98",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aac2eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "notebook_dir = os.getcwd()\n",
    "parent_dir = os.path.abspath(os.path.join(notebook_dir, '..'))\n",
    "\n",
    "if parent_dir not in sys.path:\n",
    "    sys.path.insert(0, parent_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e6fbdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from text_to_sql import (\n",
    "    TextToSQL,\n",
    "    Config,\n",
    "    LLMConfig,\n",
    "    SLConfig,\n",
    "    ContextConfig,\n",
    "    QueryConfig,\n",
    ")\n",
    "from dotenv import load_dotenv\n",
    "from datetime import datetime\n",
    "\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eee5d38",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc410d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATABASE = \"academic\"\n",
    "MODEL = \"gpt-4.1-mini\"\n",
    "PROVIDER = \"openai\"\n",
    "# TOTAL_TABLES = 14\n",
    "# TOTAL_TABLES = 15\n",
    "# TOTAL_TABLES = 18\n",
    "TOTAL_TABLES = 22"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7693cd58",
   "metadata": {},
   "source": [
    "# Load Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ab360d",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0205d44e",
   "metadata": {},
   "source": [
    "# Set Timestamp Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3848d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp = datetime.now().strftime(\"%Y_%m_%d_%H_%M\")\n",
    "output_dir = f\"../files/experiment_result/{timestamp}\"\n",
    "os.makedirs(output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb343f81",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c7101c",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_key = DATABASE.upper().replace(\"-\", \"_\")\n",
    "provider_key = PROVIDER.upper().replace(\"-\", \"_\")\n",
    "\n",
    "config = Config(\n",
    "    max_retry_attempt=5,\n",
    "    rewriter_config=LLMConfig(\n",
    "        type=\"api\",\n",
    "        model=MODEL,\n",
    "        provider=PROVIDER,\n",
    "        api_key=os.getenv(f\"API_KEY_{provider_key}\"),\n",
    "    ),\n",
    "    query_generator_config=LLMConfig(\n",
    "        type=\"api\",\n",
    "        model=MODEL,\n",
    "        provider=PROVIDER,\n",
    "        api_key=os.getenv(f\"API_KEY_{provider_key}\"),\n",
    "    ),\n",
    "    schema_linker_config=SLConfig(\n",
    "        type=\"api\",\n",
    "        model=MODEL,\n",
    "        provider=PROVIDER,\n",
    "        api_key=os.getenv(f\"API_KEY_{provider_key}\"),\n",
    "        schema_path=f\"../files/schema/{DATABASE}.txt\",\n",
    "        metadata_path=f\"../files/metadata/{DATABASE}.json\",\n",
    "    ),\n",
    "    retrieve_context_config=ContextConfig(data_path=f\"../files/dataset/dataset_{DATABASE}_example.csv\"),\n",
    "    query_executor_config = QueryConfig(\n",
    "        host=os.getenv(f\"DB_HOST_{db_key}\"),\n",
    "        database=os.getenv(f\"DB_DATABASE_{db_key}\"),\n",
    "        user=os.getenv(f\"DB_USER_{db_key}\"),\n",
    "        password=os.getenv(f\"DB_PASSWORD_{db_key}\"),\n",
    "        port=os.getenv(f\"DB_PORT_{db_key}\"),\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89fc113e",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c5f1885",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_to_sql_model = TextToSQL(config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a3f318",
   "metadata": {},
   "source": [
    "# Import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ff1ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(f\"../files/dataset/dataset_schema_linker_{DATABASE}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc5db41c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"tables_used\"] = dataset[\"tables_used\"].apply(eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4bd32ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3dc8f5e",
   "metadata": {},
   "source": [
    "# Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da550a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_table_names_from_schema(schema: dict) -> list:\n",
    "    \"\"\"\n",
    "    Given a schema dictionary (from predict_schema_only),\n",
    "    return a list of lowercase table names.\n",
    "    \"\"\"\n",
    "    return schema.get(\"potential_related_tables\", [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59966092",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_reduction(predicted_tables):\n",
    "    return 1 - (len(predicted_tables) / TOTAL_TABLES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73bccf21",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies = []\n",
    "predicted_tables_list = []\n",
    "\n",
    "for _, row in dataset.iterrows():\n",
    "    prompt = row[\"prompt\"]\n",
    "    true_tables = set(row[\"tables_used\"])\n",
    "\n",
    "    predicted_schema = text_to_sql_model.predict_schema_only(prompt)\n",
    "    predicted_tables = set(extract_table_names_from_schema(predicted_schema))\n",
    "\n",
    "    # Save predicted tables\n",
    "    predicted_tables_list.append(list(predicted_tables))\n",
    "\n",
    "    # Calculate accuracy\n",
    "    intersection_count = len(true_tables.intersection(predicted_tables))\n",
    "    total_true_tables = len(true_tables)\n",
    "\n",
    "    if total_true_tables > 0:\n",
    "        accuracy = intersection_count / total_true_tables\n",
    "    else:\n",
    "        accuracy = 1.0 if not predicted_tables else 0.0\n",
    "\n",
    "    accuracies.append(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2655543f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"predicted_tables\"] = predicted_tables_list\n",
    "dataset[\"schema_accuracy\"] = accuracies\n",
    "dataset['schema_reduction'] = dataset['predicted_tables'].apply(calculate_reduction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8564dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_accuracy = sum(accuracies) / len(accuracies)\n",
    "print(f\"Schema Prediction Accuracy (Intersection-Based): {final_accuracy:.2%}\")\n",
    "\n",
    "average_reduction = dataset['schema_reduction'].mean()\n",
    "print(f\"Average Schema Reduction: {average_reduction:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e521b9",
   "metadata": {},
   "source": [
    "# Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e793c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.to_csv(f\"{output_dir}/{MODEL}_{DATABASE}_schema_linker.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b673d12a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "534dc267",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_1 = dataset[dataset[\"schema_accuracy\"] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c15d004c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71841443",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dataset = pd.read_csv(f\"../files/dataset/dataset_{DATABASE}_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca6bdfee",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea23e85d",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_set = set(dataset_1['prompt'])\n",
    "\n",
    "def match_prompt(row):\n",
    "    alt1 = row.get(\"Alternative Prompt 1 (English)\", \"\")\n",
    "    alt2 = row.get(\"Alternative Prompt 2 (Bahasa Indonesia)\", \"\")\n",
    "    return alt1 in prompt_set and alt2 in prompt_set\n",
    "\n",
    "filtered_df = new_dataset[new_dataset.apply(match_prompt, axis=1)]\n",
    "filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa57b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(filtered_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e1af2cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_RETRIES = 5\n",
    "RETRY_DELAY = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a27e7df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import ast\n",
    "\n",
    "EA = 0\n",
    "total_questions = len(filtered_df) * 2\n",
    "results_list = []\n",
    "\n",
    "for idx, row in filtered_df.iterrows():\n",
    "    question_1 = row[\"Alternative Prompt 1 (English)\"]\n",
    "    question_2 = row[\"Alternative Prompt 2 (Bahasa Indonesia)\"]\n",
    "    answer = row[\"Answer\"]\n",
    "    expected_columns = ast.literal_eval(row[\"Expected Result\"])\n",
    "\n",
    "    for prompt_id, question in enumerate([question_1, question_2], start=1):\n",
    "        print(f\"\\nProcessing Question {idx + 1}.{prompt_id}: {question}\")\n",
    "        result = None\n",
    "\n",
    "        for attempt in range(1, MAX_RETRIES + 1):\n",
    "            try:\n",
    "                result = text_to_sql_model.generate_baseline(user_prompt=question)\n",
    "                break\n",
    "            except Exception as e:\n",
    "                print(f\"[Attempt {attempt}] Failed to generate SQL: {e}\")\n",
    "                if attempt < MAX_RETRIES:\n",
    "                    time.sleep(RETRY_DELAY)\n",
    "                else:\n",
    "                    print(\"Max retries reached. Setting result as 'ERROR'\")\n",
    "                    result = \"ERROR\"\n",
    "\n",
    "        print(f\"Generated SQL Query: {result}\")\n",
    "\n",
    "        try:\n",
    "            acc = text_to_sql_model.evaluate(query=result, true_query=answer, expected_columns=expected_columns)\n",
    "        except Exception as e:\n",
    "            print(f\"Evaluation failed: {e}\")\n",
    "            acc = 0.0\n",
    "\n",
    "        print(f\"Execution Accuracy: {acc:.4f}\")\n",
    "\n",
    "        results_list.append({\n",
    "            \"Question ID\": f\"{idx + 1}.{prompt_id}\",\n",
    "            \"Question\": question,\n",
    "            \"Generated SQL Query\": result,\n",
    "            \"Expected SQL Query\": answer,\n",
    "            \"Execution Accuracy\": acc\n",
    "        })\n",
    "\n",
    "        EA += acc\n",
    "\n",
    "# Calculate final execution accuracy\n",
    "final_accuracy = EA / total_questions if total_questions > 0 else 0\n",
    "print(f\"\\nFinal Execution Accuracy: {final_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc4a0ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results_partial_baseline = pd.DataFrame(results_list)\n",
    "df_results_partial_baseline.to_csv(f\"{output_dir}/{MODEL}_{DATABASE}_partial_baseline.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5665333",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import ast\n",
    "\n",
    "EA = 0\n",
    "total_questions = len(filtered_df) * 2\n",
    "results_list = []\n",
    "\n",
    "for idx, row in filtered_df.iterrows():\n",
    "    question_1 = row[\"Alternative Prompt 1 (English)\"]\n",
    "    question_2 = row[\"Alternative Prompt 2 (Bahasa Indonesia)\"]\n",
    "    answer = row[\"Answer\"]\n",
    "    expected_columns = ast.literal_eval(row[\"Expected Result\"])\n",
    "\n",
    "    for prompt_id, question in enumerate([question_1, question_2], start=1):\n",
    "        print(f\"\\nProcessing Question {idx + 1}.{prompt_id}: {question}\")\n",
    "        result = None\n",
    "\n",
    "        for attempt in range(1, MAX_RETRIES + 1):\n",
    "            try:\n",
    "                result = text_to_sql_model.predict_sql_schema_only(user_prompt=question)\n",
    "                break\n",
    "            except Exception as e:\n",
    "                print(f\"[Attempt {attempt}] Failed to generate SQL: {e}\")\n",
    "                if attempt < MAX_RETRIES:\n",
    "                    time.sleep(RETRY_DELAY)\n",
    "                else:\n",
    "                    print(\"Max retries reached. Setting result as 'ERROR'\")\n",
    "                    result = \"ERROR\"\n",
    "\n",
    "        print(f\"Generated SQL Query: {result}\")\n",
    "\n",
    "        try:\n",
    "            acc = text_to_sql_model.evaluate(query=result, true_query=answer, expected_columns=expected_columns)\n",
    "        except Exception as e:\n",
    "            print(f\"Evaluation failed: {e}\")\n",
    "            acc = 0.0\n",
    "\n",
    "        print(f\"Execution Accuracy: {acc:.4f}\")\n",
    "\n",
    "        results_list.append({\n",
    "            \"Question ID\": f\"{idx + 1}.{prompt_id}\",\n",
    "            \"Question\": question,\n",
    "            \"Generated SQL Query\": result,\n",
    "            \"Expected SQL Query\": answer,\n",
    "            \"Execution Accuracy\": acc\n",
    "        })\n",
    "\n",
    "        EA += acc\n",
    "\n",
    "# Calculate final execution accuracy\n",
    "final_accuracy = EA / total_questions if total_questions > 0 else 0\n",
    "print(f\"\\nFinal Execution Accuracy: {final_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa72927",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results_partial_schema_linker = pd.DataFrame(results_list)\n",
    "df_results_partial_schema_linker.to_csv(f\"{output_dir}/{MODEL}_{DATABASE}_partial_schema_linker.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
